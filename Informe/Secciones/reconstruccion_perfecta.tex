\paragraph{}
El objetivo de esta sección será aclarar el significado de reconstrucción perfecta. Analizaremos las implicaciones que tiene la reconstrucción perfecta en el dominio del tiempo, en el dominio modulado y en el dominio polifásico. Los análisis serán desarrollados para bancos de filtros de dos canales, pero se darán las fórmulas equivalentes para bancos genéricos de N canales.
\paragraph{}
El significado de reconstrucción perfecta, en el contexto de bancos de filtros, significa que la salida es una version retrasada y posiblemente escalada de la entrada:

\begin{equation}
	\hat{X}(z) = c z^{-k} X(z)
	\label{eq:recons_perf}
\end{equation}

Analizaremos el significado de esta definición en los distintos dominios antes dichos.

\subsection{Análisis en el Dominio del Tiempo}

\Mati{Poner lo que dice de la pagina 130 a 133.}

Primero, se expresan los coeficientes transformados del siguiente modo:
	\begin{equation}
		\begin{pmatrix}
			\vdots \\
			y_0[0] \\
			y_1[0] \\
			y_0[1] \\
			y_1[1] \\
			\vdots \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\vdots \\
			X[0] \\
			X[1] \\
			X[2] \\
			X[3] \\
			\vdots \\
		\end{pmatrix}
		= T_a 
		\begin{pmatrix}
			\vdots \\
			x[0] \\
			x[1] \\
			x[2] \\
			x[3] \\
			\vdots \\
		\end{pmatrix}
	\end{equation}
	
	\begin{equation}
		T_a=
		\begin{bmatrix*}[c]
			\vdots & \vdots & \vdots & & \vdots & \vdots & \vdots\\
			h_0[L-1] & h_0[L-2] & h_0[L-3] & \hdots & h_0[0] & 0 & 0 \\
			h_1[L-1] & h_1[L-2] & h_1[L-3] & \hdots & h_1[0] & 0 & 0 \\
			0 & 0 & h_0[L-1] & \hdots & h_0[2] & h_0[1] & h_0[0] \\
			0 & 0 & h_1[L-1] & \hdots & h_1[2] & h_1[1] & h_1[0] \\
			\vdots & \vdots & \vdots & & \vdots & \vdots & \vdots \\
		\end{bmatrix*}
	\end{equation}

Asumiendo que los filtros $h_i$ son FIR de largo $L=2K$, la matriz $T_a$ puede refactorizarse del siguiente modo:

	\begin{equation}
		T_a=
		\begin{bmatrix*}[c]
			 & \vdots & \vdots &  & \vdots & \vdots & &\\
			\hdots & A_0 & A_1 & \hdots & A_{K-1} & 0 & \hdots  \\
			\hdots & 0 & A_0 & \hdots & A_{K-2} & A_{K-1} & \hdots  \\
			 & \vdots & \vdots &  & \vdots & \vdots & &\\		
		\end{bmatrix*}
	\end{equation}

Siendo cada una de las submatrices $A_i$

	\begin{equation}
		A_i=
		\begin{bmatrix*}[c]
			h_0[2K-1-2i] & h_0[2K-2-2i] \\
			h_1[2K-1-2i] & h_1[2K-2-2i] \\
		\end{bmatrix*}
	\end{equation}
	
\Emi{Falta agregar el speech y alguna que otra ecuación}

\subsection{Análisis en el Dominio Modulado}

	Para este caso, la idea es expresar a la señal submuestrada en dos $X'(z)$ como
		\begin{equation}
			X'(z)=\frac{1}{2}[X(z^{1/2})+X(-z^{1/2})]
		\end{equation}

	Luego, al sobremuestrear por dos a $X'(z)$, se expresa a $X''(z)=X'(z^2)$ como
		\begin{equation}
			X''(z)=\frac{1}{2}[X(z)+X(-z)]
		\end{equation}

	Entonces la señal filtrada por el filtro $H_0(z)$, de acuerdo al esquema de la Figura \ref{fig:banco_de_2}, resulta
		\begin{equation}
			\frac{1}{2}[H_0(z)X(z)+H_0(-z)X(-z)]
		\end{equation}

	Filtrando con $G_0(z)$ a continuación, se obtiene $X_0(z)$
		\begin{equation}
			X_0(z)=\frac{1}{2}G_0(z)[H_0(z)X(z)+H_0(-z)X(-z)]
		\end{equation}

	Análogamente, en la otra rama del esquema se obtiene $X_1(z)$
		\begin{equation}
			X_1(z)=\frac{1}{2}G_1(z)[H_1(z)X(z)+H_1(-z)X(-z)]
		\end{equation}

	La salida reconstruida $\hat{X}(z)$ se obtiene como la suma de ambas ramas. Expresado de forma matricial:
		\begin{equation}
			\hat{X}(z)
			=
			X_0(z)+X_1(z)
			=
			\frac{1}{2}
			\begin{pmatrix}
				G_0(z) & G_1(z)
			\end{pmatrix}
			\underbrace{
				\begin{pmatrix}
					H_0(z) & H_0(-z) \\
					H_1(z) & H_1(-z)
				\end{pmatrix}
			}_{\vect{H}_m(z)}
			\underbrace{
				\begin{pmatrix}
					X(z) X(-z)
				\end{pmatrix}
			}_{\vect{x}_m(z)}
		\label{eq:modmat}
		\end{equation}

	Donde $\vect{H}_m(z)$ es la matriz de análisis de modulación, que tiene versiones moduladas de los filtros de análisis, y $\vect{x}_m(z)$, que son las versiones moduladas de $X(z)$

	\graficarPNG{analisis_modulado}{Análisis en el dominio modulado del banco de filtros}{fig:analisis_mod}

	Para obtener las versiones submuestreadas $Y_0(z)$ e $Y_1(z)$:
		\begin{equation}
			\begin{pmatrix}
				Y_0(z) \\
				Y_1(z)
			\end{pmatrix}
			=
			\frac{1}{2}
			\begin{pmatrix}
				H_0(z^{1/2}) & H_0(-z^{1/2}) \\
				H_1(z^{1/2}) & H_1(-z^{1/2}) \\
			\end{pmatrix}
			\begin{pmatrix}
				X(z^{1/2}) \\
				X(-z^{1/2})
			\end{pmatrix}
		\end{equation}
	
	O bien, de forma matricial, con las matrices $\vect{H}_m(z)$ y $\vect{x}_m(z)$ anteriormente definidas
		\begin{equation}
			\vect{y}(z)=\frac{1}{2}\vect{H}_m(z^{1/2})\vect{x}_m(z^{1/2})
		\end{equation}

	Para comprobar que en efecto se trata de un filtro de reconstrucción perfecta, debe cumplir que
		\begin{equation}
			\hat{X}(z)=X(z)
		\end{equation}

	Observando nuevamente la ecuación \eqref{eq:modmat}, esto implica que
		\begin{align}
			G_0(z)H_0(z)+G_1(z)H_1(z)=2		\label{eq:modcond}\\
			G_0(z)H_0(-z)+G_1(z)H_1(-z)=0		\label{eq:modcond2}
		\end{align}

	O bien, matricialmente
		\begin{equation}
			\begin{pmatrix}
				G_0(z) & G_1(z)
			\end{pmatrix}
			\vect{H}_m(z)
			=
			\begin{pmatrix}
				2 & 0
			\end{pmatrix}
		\end{equation}

	Asumiendo que $\vect{H}_m(z)$ es invertible, se resuelve para $G_0(z)$ $G_1(z)$
		\begin{equation}
			\begin{pmatrix}
				G_0(z) \\
				G_1(z)
			\end{pmatrix}
			=
			\frac{2}{\det{(\vect{H}_m(z))}}
			\begin{pmatrix}
				H_1(-z) \\
				-H_0(-z)
			\end{pmatrix}
		\end{equation}
		
	Ahora, definiendo $P(z)=G_0(z)H_0(z)$
		\begin{equation}
			P(z)=\frac{2}{\det{(\vect{H}_m(z))}}H_1(-z)H_0(z)
		\end{equation}

	De igual modo, teniendo en cuenta que $-\det{(\vect{H}_m(z))}=\det{(\vect{H}_m(-z))}$, hallamos que
		\begin{equation}
			G_1(z)H_1(z)=\frac{2}{\det{(\vect{H}_m(-z))}}H_0(-z)H_1(z)=P(-z)
		\end{equation}

	Entonces, reemplazando a $P(z)$ y $P(-z)$ en  \eqref{eq:modcond}
		\begin{equation}
			P(z)+P(-z)=2
		\end{equation}
	
	Por lo tanto, todos los coeficientes pares distintos de cero se anulan, y $P(0)=1$. $P(z)$ entonces adopta la forma
		\begin{equation}
			P(z)=1+\sum_{k\in\mathbb{Z}} p[2k+1]z^{-(2k+1)}
		\end{equation}
	
	Observando nuevamente \eqref{eq:modcond} y \eqref{eq:modcond2}, y transformándolas al dominio del tiempo nuevamente a través de la propiedad de modulación/convolución: \Emi{Modulación o convolución?}
		\begin{equation}
			\sum_{k\in\mathbb{Z}}g_0[k]h_0[n-k]+(-1)^n\sum_{k\in\mathbb{Z}}g_0[k]h_0[n-k]=2\delta[n]
			\label{eq:modconvol}
		\end{equation}

	Al cancelarse los términos impares, la Ecuación \eqref{eq:modconvol} puede reescribirse como
		\begin{equation}
			\sum_{k\in\mathbb{Z}}g_0[k]h_0[2n-k]=\delta[n]
		\end{equation}

	Y presentado como un producto interno:
		\begin{equation}
			\langle g_0[k],h_0[2n-k] \rangle=\delta[n]
		\end{equation}

	Vinculándolo con la Ecuación \eqref{eq:biortcond}, que establece la condición de biortogonalidad, se establecen las restricciones siguientes:

	\begin{align}
		\langle \hat{\phi_1[k]},\phi_{2n+1}[k] \rangle = \delta[n]	\\
		\langle \hat{\phi_0[k]},\phi_{2n+1}[k]\rangle	= 0	\\
		\langle \hat{\phi_1[k]},\phi_{2n}[k]\rangle = 0
	\end{align}

	Es decir, hay un estrecho vínculo entre la condición de reconstrucción perfecta y que las funciones base deban ser biortogonales.

\subsection{Análisis en el Dominio Polifásico}

	Al igual que en el análisis del dominio modulado, se descompone el sistema periódicamente 
	invariante en varios subsistemas tiempo invariantes. En este caso se descomponen los sistemas en sus componentes polifásicas. 
	Dicha descomposición consta de versiones submuestreadas y con diferentes fases, cuya suma y corrección de fase reconstruye la sucesión inicial.
	%En una trasformación polifásica, se descompone la sucesión en $N$ sucesiones que cada una es retardada $i$ unidades y submuestreada en $N$ con respecto a la original. 
	Formalmente se definen las componentes polifásicas en el tiempo según
		\begin{equation}
			x_i[n] = x[nN+i]
			\label{eq:poly_comp}
		\end{equation}
		
		y en transformada $z$ como 
		\begin{equation} %2.5.20
			X(z)= \sum^{N-1}_{i=0} z^{-i} \, X_i(z^N)
			\label{eq:poly_comp_z}
		\end{equation}
		
		con
		\begin{equation} %2.5.21
			X_i(z)=\sum^{\infty}_{n=-\infty} x[nN+ i]\, z^{-n}
			\label{eq:poly_comp_i_z}
		\end{equation}
%	Formalmente se definen las componentes polifásicas en el tiempo según \eqref{eq:poly_comp} y en transformada $z$ según \eqref{eq:poly_comp_z} con \eqref{eq:poly_comp_i_z}.

	\graficarPNG{polifasico_n_2}{Descomposición polifásica como banco de filtros.}{fig:poli_n_2}
	Se propone probar la reconstrucción perfecta, es decir $x = \hat{x}$ o equivalentemente $X(z) = \hat{X}(z)$. Se presenta el sistema de la Figura \ref{fig:poli_n_2}, considerando $N=2$ y con filtros de análisis $\vect{H}_p(z)$ y síntesis $\vect{G}_p(z)$. 

	Como $N=2$ la transformada $z$ en el dominio polifásico de $x$ resulta:
		\begin{equation}
			X(z) = X_0 (z^2) + z^{-1}\, X_1 (z^2)
			\label{eq:xp_z}
		\end{equation}
	mientras que la componente i-ésima del filtro $H_p$ (calculada al igual que \eqref{eq:poly_comp_z} y \eqref{eq:poly_comp_i_z} salvo la inversión de fase) se simplifica a:
		\begin{equation}
			H_i(z) = H_{i0} (z^2) + z\, H_{i1} (z^2)
			\label{eq:hpi_z}
		\end{equation}

		Así se encuentra $\vect{y}_p(z)$ como:
		\begin{equation}
		\underbrace{\begin{bmatrix} Y_0(z)\\[0.3em] Y_1(z) \end{bmatrix}}_{\vect{y}_p(z)} = \underbrace{\begin{bmatrix} H_{00}(z) & H_{01}(z) \\[0.3em] H_{10}(z) & H_{11}(z) \end{bmatrix}}_{\vect{H}_p(z)} \underbrace{\begin{bmatrix} X_0(z) \\[0.3em] X_1(z) \end{bmatrix}}_{\vect{x}_p(z)}
			\label{eq:in_poly_z}
		\end{equation}

		Del mismo modo, pero en orden inverso al realizar el sobremuestreo primero, se obtiene $\hat{X}(z)$ a partir de $\vect{y}_p(z)$ como:
		\begin{equation}
		\hat{X}(z) = \begin{bmatrix} 1 & z^{-2} \end{bmatrix} \underbrace{\begin{bmatrix} G_{00}(z^2) & G_{01}(z^2) \\[0.3em] G_{10}(z^2) & G_{11}(z^2) \end{bmatrix}}_{\vect{G}_p(z^2)} \underbrace{\begin{bmatrix} Y_0(z^2)\\[0.3em] Y_1(z^2) \end{bmatrix}}_{\vect{y}_p(z^2)} 
			\label{eq:out_poly_z}
		\end{equation}

		Reemplazando \eqref{eq:in_poly_z} en \eqref{eq:out_poly_z}:
		\begin{equation*}
		\hat{X}(z) = \begin{bmatrix} 1 & z^{-2} \end{bmatrix}\; \underbrace{\vect{G}_p(z^2) \; \vect{H}_p(z^2)}_{\vect{T}_p(z^2)} \; \vect{x}_p(z^2)
		\end{equation*}

		Se puede ver que si $\vect{T}_p(z^2) = \vect{I}$, la reconstrucción es perfecta obteniéndose el mismo resultado que \eqref{eq:xp_z}. Para el caso no trivial de la matriz identidad, si la matriz transferencia consta de retardos y factores de amplificación/atenuación, la reconstrucción sigue siendo perfecta como se establece en \eqref{eq:recons_perf}.

	\subsection{Resultados en Bancos de Filtros}
	%\newtheorem{prepo}{Preposición}
	%\theoremstyle{definition}
	El mayor problema de las reconstrucciones es el \emph{aliasing}. Dada la naturaleza de los sistemas periódicamente invariantes, la salida depende no sólo de la entrada sino también de su versión modulada por $(-1)^n$. En términos de la transformada $z$, dependen de $X(z)$ y también de $X(-z)$ imponiendo una distorsión no-harmónica llamada \emph{aliasing}. Por lo tanto, es de gran interés encontrar reconstrucciones libres \emph{aliasing}\\

	%\begin{prepo}
		Se prueba que se puede cancelar el aliasing si sólo si la matriz transferencia $\vect{T}_p(z)$ es seudocirculante. Ésto quiere decir que las filas de la matriz tienen los mismos coeficientes salvo un \emph{shift} lateral, y los coeficientes de la triagular inferior se ven multiplicados por $z$. En $2\times 2$ una matriz de transferencia típica tiene la forma:
		\begin{equation*}
		\vect{T}_p(z) = \begin{bmatrix} F_0 (z) & F_1(z) \\[0.3em] z\, F_1(z) & F_0(z) \end{bmatrix}
		\end{equation*}
			
		Un corolario interesante de la proposicion anterior es que $\vect{T}_p(z)$ tiene que ser una matriz de retardo $d$ seudocirculante. Matemáticamente se ve como:
		\begin{equation*}
		\begin{cases}\vect{T}_p(z)=z^{-k}\begin{bmatrix}1&0\\0&1\end{bmatrix}, &\text{si }d=2k \\[0.5cm]
		\vect{T}_p(z)=z^{-k-1}\begin{bmatrix}0&1\\z&0\end{bmatrix}, &\text{si }d=2k+1\end{cases}
		\end{equation*}
	%\end{prepo}

		Otra preposición que se obtiene es que la reconstrucción es perfecta y sin aliasing si sólo si se hace un submuestreo de tamaño $N$ y $\vect{H}_p(z)$ es de rango $N$ o rango completo (equivalente a pedir que el determinante no sea nulo). La prueba es trivial dado que si se elige a $\vect{G}_p(z)$ como la matriz de cofactores de $\vect{H}_p(z)$, entonces $\vect{T}_p(z) = \det\left(\vect{H}_p(z)\right)\,\vect{I}$ que es seudocirculante.\\

		Un caso interesante es determinar cuándo para filtros FIR la reconstrucción es perfecta. Ésto sucede si solo si el determinante de $\vect{H}_p(z)$ es un retardo puro.\\
		\indent Tomando $\vect{H}_p(z)$ como un retardo puro y $\vect{G}_p(z)$ como la matriz de cofactores de $\vect{H}_p(z)$ se obtiene reconstrucción perfecta con filtros FIR. Por otro lado, si la reconstrucción es perfecta con filtros FIR, $\vect{T}_p(z)$ tiene que ser un retardo puro pseudocirculante: 
		
		\begin{equation*}
			\det(\vect{T}_p(z)) = \det(\vect{G}_p(z)) \det(\vect{H}_p(z)) = z^{-d}
		\end{equation*}
		
		Como $\det(\vect{T}_p(z))$ tiene $d$ polos en 0 y las síntesis $\det(\vect{G}_p(z))$ tiene que tener ceros únicamente (para ser FIR), entonces necesariamente $\det(\vect{H}_p(z)$) no puede tener ceros (excepto en el origen e infinito).\\

	% ESTO SE SACO PARA QUE GIRIBET NO NOS PREGUNTE COSAS MAS AVANZADAS DE ESTO.

		%Por último se destaca el método de QMF (\emph{quadrature mirror filters}) que cancela el \emph{aliasing} en filtros de 2 canales. Como se vio en la sección de dominio modulado para una reconstrucción perfecta libre de \emph{aliasing} se requiere cumplir las Ecuaciones (ecuación 3.2.15 y 16 multiplicada por un delay $z^{-l}$). La solución propuesta entonces se resume en:
		%\begin{equation*}
		%	H^2_0 (z) - H^2_0 (-z) = 2\, z^{-l}
		%\end{equation*}

		%Para el caso particular FIR, ésta condición no se puede satisfacer excepto que se utilice únicamente un filtro de Haar causal. Si se considerasen filtros de fase lineal dicho resultado no se puede obtener pero se puede aproximar.


\Mati{La idea sería dar las formulas equivalentes para N canales. Estan en las páginas 184 a 188.}

